m
t(x)[upper.tri(x)]
t(x)[lower.tri(x)]
y <- matrix(1:16, nrow=4, ncol=4)
y
y1 <- t(matrix(1:16, nrow=4, ncol=4))
y1
diag(y) <- 0
diag(x) <- 0
y1 <- t([upper.tri(y)])
y1 <- t(y)[upper.tri(y)]
y1
x <- matrix(1:16, nrow=4, ncol=4)
diag(x) <- 0
m = x
m[upper.tri(m)] <- (x[upper.tri(x)] + x[lower.tri(x)])
m[lower.tri(m)] <- (t(x)[upper.tri(x)] + t(x)[lower.tri(x)])
m
x <- matrix(1:16, nrow=4, ncol=4)
y <- matrix(1:16, nrow=4, ncol=4)
y1 <- t(y)[upper.tri(y)]
diag(y) <- 0
diag(x) <- 0
m = x
m[upper.tri(m)] <- (x[upper.tri(x)] + x[lower.tri(x)])
m[lower.tri(m)] <- (t(x)[upper.tri(x)] + t(x)[lower.tri(x)])
x <- matrix(1:100, nrow=10, ncol=10)
x
diag(x) <- 0
sum(x[upper.tri(x)], x[lower.tri(x)])
sumofx <- sum(x[upper.tri(x)], x[lower.tri(x)])
sumofx
?sum
sumofx <- sum([upper.tri(x)], [lower.tri(x)])
sum.x <- (x[upper.tri(x)] + x[lower.tri(x)])
sum.x
small <- t(sum.x)
small
sum.x <- (x[upper.tri(x)] + x[lower.tri(x)])
sum.x
upper.tri(x)
x[upper.tri(x)]
x <- matrix(1:100, nrow=10, ncol=10)
x
diag(x) <- 0
sum.x <- (x[upper.tri(x)] + x[lower.tri(x)])
upper.tri(x)
x[upper.tri(x)]
x[upper.tri(x)] <- sum.x
print(x)
m
m = x
x <- matrix(1:100, nrow=10, ncol=10)
x
diag(x) <- 0
m = x
sum.x <- (x[upper.tri(x)] + x[lower.tri(x)])
upper.tri(x)
x[upper.tri(x)] <- sum.x
print(x)
m
x <- matrix(1:100, nrow=10, ncol=10)
x
diag(x) <- 0
m = x
sum.x <- (x[upper.tri(x)] + x[lower.tri(x)])
lower.tri(x)
x[lower.tri(x)] <- sum.x
print(x)
m
upper.tri(x)
x
x[upper.tri(x)]
x[lower.tri(x)]
x <- matrix(1:16, nrow=4, ncol=4)
x
diag(x) <- 0
m = x
sum.x <- (x[upper.tri(x)] + x[lower.tri(x)])
upper.tri(x)
x[upperr.tri(x)] <- sum.x
print(x
print(x)
print(x)
m
sum.x
upper.tri(x)
x[upper.tri(x)] <- sum.x
print(x)
x <- matrix(1:16, nrow=4, ncol=4)
x
diag(x) <- 0
library(mapproj)
library(mapdata)
library(maptools)
library(dismo)
library(rJava)
library(rgbif)
########################################SETTING UP YOUR DATA###################################
Apicea_raw <- gbif(genus = 'Aphaenogaster', species = 'picea') #this function will pull
#up all of the records in gbif of A. picea, and we have assigned a name to it.
Apicea_raw[,c('lat','lon')] #now we'll create a vector which only calls up lat and long information.
## you will notice that some records don't have any useful information ("NA")
#to omit these use the "na.omit()" function to clean it up.
na.omit(Apicea_raw[,c('lat','lon')])
#and re-assign a name
Apicea <- na.omit(Apicea_raw[,c('lat','lon')])
#you can check that everything was ommited by printing. print(Apice# notice we're limiting the extent of the map to focus on the Mojave Desert region
#now checkout the ranges so you know what coordinate ranges to use.
range(Apicea[ ,'lon'])
range(Apicea[ , 'lat'])
#Then plot these points to check them ...
data(stateMapEnv)
###########################PLOTTING PRESENCE AND ABSENCE POINTS#################################
# notice we're limiting the extent of the map to focus on the Mojave Desert region
#here, c(-99.2, -63) will set your longitudinal boudries, namely your west and east boundries.
#whereas c(23.6, 45.5) will set your latitude boundries, in other words your north and south limits
plot(c(-99.2, -63), c(23.6, 45.5), mar=par("mar"), xlab="longitude", ylab="latitude", xaxt="n", yaxt="n", type="n", main="Joshua tree presence data")
rect(par("usr")[1],par("usr")[3],par("usr")[2],par("usr")[4], col="lightcyan")
map("state", xlim=c(-99.2, -63), ylim=c(23.6, 45.5), fill=T, col="honeydew", add=T)
points(Apicea$lon, Apicea$lat, col="darkolivegreen4", pch=20, cex=0.5)
# add some axes
axis(1,las=1)
axis(2,las=1)
#Outline the graph
box()
# create sequences of latitude and longitude values to define the grid
longrid = seq(-99.2,-63,0.05)
latgrid = seq(23.6, 45.5,0.05)
# identify points within each grid cell, draw one at random
subs = c()
for(i in 1:(length(longrid)-1)){
for(j in 1:(length(latgrid)-1)){
gridsq = subset(Apicea, lat > latgrid[j] & lat < latgrid[j+1] & lon > longrid[i] & lon < longrid[i+1])
if(dim(gridsq)[1]>0){
subs = rbind(subs, gridsq[sample(1:dim(gridsq)[1],1 ), ])
}
}
}
dim(subs) # confirm that you have a smaller dataset than you started with
# define circles with a radius of 50 km around the subsampled points
x=circles(subs[,c("lon","lat")], d=50000, lonlat=T)
plot(x@polygons, axes=T, col=rgb(0,0,0,0.1), border=NA, add=T)
#if you don't have the rgeos package already installed, it is likely that you will
#run into an error. To install it use this command: install.packages("rgeos) and
# install.packages("rgdal")
# draw random points that must fall within the circles in object x
bg = spsample(x@polygons, 1000, type='random', iter=1000)
library(rdgal)
library("rgeos", lib.loc="~/Library/R/3.2/library")
library("rgdal", lib.loc="~/Library/R/3.2/library")
bg = spsample(x@polygons, 1000, type='random', iter=1000)
points(bg,col="khaki4",pch=1,cex=0.3)
######################################HANDLING CLIMATE DATA#####################################
require(raster)
BClim = getData("worldclim", var="bio", res=2.5, path="/Users/annacalderon/Desktop/gENM/data")
#if the data is unwiedly, crop it.
YbrevRange = extent(-99.2,-63,23.6, 45.5) # define the extent
BClim = crop(BClim, YbrevRange)
writeRaster(BClim, filename="/Users/annacalderon/Desktop/gENM/data", overwrite=T)
#....aaaaand, reload it from the file that contains the croped BClim data
BClim = brick("/Users/annacalderon/Desktop/gENM/data/data.grd")
#Now you can start plotting your data! YEYYYYYYYY.
#########################ANNUAL MEAN TEMPERATURE (C X 10)#####################################
# this format plots the first (of 19) variables stored in BClim; change the 1 to 2-19 for the others
plot(BClim, 1, cex=0.5, legend=T, mar=par("mar"), xaxt="n", yaxt="n", main="Annual mean temperature (ºC x 10)")
map("state", xlim=c(-99.2,-63), ylim=c(23.6,45.5), fill=F, col="honeydew", add=T)
# state names
text(x=-99.06, y=31.72, "Texas", col="black", cex=.3)
text(x=-97.06, y=35.02, "Oklahoma", col="black", cex=.3)
text(x=-97.7, y=38, "Texas", col="black", cex=.3)
text(x=-98, y=41 , "Nebraska", col="black", cex=.3)
text(x=-99, y=44 , "South Dakota", col="black", cex=.3)
text(x=-100, y=47 , "North Dakota", col="black", cex=.3)
text(x=-94, y=46 , "Minnesota", col="black", cex=.3)
text(x=-93, y=42 , "Iowa", col="black", cex=.3)
text(x=-93, y=38 , "Missouri", col="black", cex=.3)
text(x=-89, y=33 , "Mississippi", col="black", cex=.3)
text(x=-83, y= 33, "Gerogia", col="black", cex=.3)
text(x=-81, y=27 , "Florida", col="black", cex=.3)
text(x=-92, y= 35, "Arkansas", col="black", cex=.3)
text(x=-85, y=37.5 , "Kentucky", col="black", cex=.3)
text(x=-86, y=35, "Tennessee", col="black", cex=.3)
text(x=-86, y= 32, "Alabama", col="black", cex=.3)
text(x=-89, y=40 , "Illinois", col="black", cex=.3)
text(x=-85, y= 43.5, "Michigan", col="black", cex=.3)
text(x=-83, y=40 , "Ohio", col="black", cex=.3)
text(x=-89.5, y=44 , "Wisconsin", col="black", cex=.3)
text(x=-78, y=41 , "Pennsylvania", col="black", cex=.3)
text(x=-79, y=37, "Virginia", col="black", cex=.3)
text(x=-80, y= 34, "South Carolina", col="black", cex=.3)
text(x=-87, y=36 , "Tennessee", col="black", cex=.3)
text(x=-75, y=43 , "New York", col="black", cex=.3)
text(x=-86, y=40 , "Indiana", col="black", cex=.3)
text(x=-81, y=38 , "West Virginia", col="black", cex=.3)
text(x=-78, y=35 , "North Carolina", col="black", cex=.3)
points(Apicea$lon, Apicea$lat, col="darkolivegreen4", pch=20, cex=0.5)
#...annnnddd...your pseudo-absence points. yus.
points(bg,col="snow",pch=1,cex=0.2)
# add axes
axis(1,las=1)
axis(2,las=1)
#and a box
box()
#################################PULLING BIOCLIM VALUE######################################
##################################????????????????????######################################
# for the subsampled presence points
Ybrev_bc = extract(BClim, subs[,c("lon","lat")])
# for the pseudo-absence points
bg_bc = extract(BClim, bg)
#You’ll probably want to build some useful dataframes
#with two columns of coordinates followed by the 19 bioclim variables.
#First, for the presence points:
Ybrev_bc = data.frame(lon=subs$lon, lat=subs$lat, Ybrev_bc)
#And then for the pseudo-absences:
bgpoints = bg@coords
colnames(bgpoints) = c("lon","lat")
bg_bc = data.frame(cbind(bgpoints,bg_bc))
length(which(is.na(bg_bc$bio1))) # double-check for missing data
bg_bc = bg_bc[!is.na(bg_bc$bio1), ] # and pull out the missing lines
group_p = kfold(Ybrev_bc, 5) # vector of group assignments splitting the Ybrev_bc into 5 groups
group_a = kfold(bg_bc, 5) # ditto for bg_bc
####################################  BUILDIG YOUR SDM  ############################################
test=3
train_p = Ybrev_bc[group_p!=test, c("lon","lat")]
train_a = bg_bc[group_a!=test, c("lon","lat")]
test_p = Ybrev_bc[group_p==test, c("lon","lat")]
test_a = bg_bc[group_a==test, c("lon","lat")]
me = maxent(BClim, p=train_p, a=train_a)
e = evaluate(test_p, test_a, me, BClim)
e
pred_me = predict(me, BClim) # generate the predictions
# make a nice plot
plot(pred_me, 1, cex=0.5, legend=T, mar=par("mar"), xaxt="n", yaxt="n", main="Predicted Presence of A. picea")
map("state", xlim=c(-99.2,-63), ylim=c(23.6,45.5), fill=F, col="black", add=T)
# state names
text(x=-99.06, y=31.72, "Texas", col="black", cex=.3)
MA_trees <- read.csv(https://raw.githubusercontent.com/HarvardForest/Rworkshop/master/data/CleanedMATrees.csv)
MA_trees <- read.csv("https://raw.githubusercontent.com/HarvardForest/Rworkshop/master/data/CleanedMATrees.csv")
WA_trees <- read.csv("https://raw.githubusercontent.com/HarvardForest/Rworkshop/master/data/CleanedWATrees.csv")
View(MA_trees)
typeof(MA_trees)
colnames(MA_trees)
MA_trees <- MA_trees[, -1:14]
MA_trees <- MA_trees[, "-1:14"]
MA_trees[, 1:14]
colnames(MA_trees[, 1:14])
MA_trees[, 1:14] <- NULL
?dat
MA_trees[, (1:14):=NULL]
MA_trees[,c("col1","col14"):=NULL]
MA_Trees <- matrix(MA_trees)
typeof(MA_trees)
MA_arboles <- matrix(MA_trees)
typeof(MA_arboles)
MA_arboles <- as.matrix(MA_trees)
typeof(MA_arboles)
hi<-MA_arboles[, (1:14):=NULL]
install.packages(plyr)
install.packages('plyr')
MA_trees
rm(MA_Trees)
ddply(MA_trees, .(INVYR), summerize, TotalCarbon=sum(CARBON_AG))
library(plyr)
ddply(MA_trees, .(INVYR), summerize, TotalCarbon=sum(CARBON_AG))
ddply(MA_trees, .(INVYR), summarize, TotalCarbon=sum(CARBON_AG))
TotalCarbon
meow <- ddply(MA_trees, .(INVYR), summarize, TotalCarbon=sum(CARBON_AG))
View(meow)
colnames(meow)
meow <- ddply(MA_trees, .(INVYR), summarise, TotalCarbon=sum(CARBON_AG))
View(meow)
meow <- ddply(WA_trees, .(INVYR), summarise, TotalCarbon=sum(CARBON_AG))
View(meow)
meow <- ddply(MA_trees, .(INVYR), summarise, TotalCarbon=sum(CARBON_AG))
View(meow)
View(woof)
woof <- ddply(WA_trees, .(INVYR), summarise, TotalCarbon=sum(CARBON_AG))
View(woof)
meow <- ddply(MA_trees, .(INVYR), summarise, TotalCarbon=sum(CARBON_AG, na.rm=TRUE))
View(meow)
plot(meow)
plot(meow, pch=20, cex=0.5, col="blue")
plot(meow, pch=20, cex=0.5, col="aliceblue")
plot(meow, pch=20, cex=0.5, col="pink")
plot(meow, pch=20, cex=1.0, col="pink")
plot(meow, pch=20, cex=1.0, col="pink", lwd=0.5)
plot(meow, pch=20, cex=1.0, col="pink", lwd=0.5)
plot(meow, pch=20, cex=1.0, col="pink", lwd=0.5)
plot(meow, type="o", pch=20, cex=1.0, col="pink", )
plot(meow, type=1, pch=20, cex=1.0, col="pink", )
points(woof, pch=20, cex=1.0, col="olivegreen")
points(woof, pch=20, cex=1.0, col="olive")
points(woof, pch=20, cex=1.0, col="green")
plot(meow, pch=20, cex=1.0, col="pink" )
points(woof, pch=20, cex=1.0, col="green")
points(woof, pch=20, cex=1.0, col="green")
plot(woof, pch=20, cex=1.0, col="green" )
points(meow, pch=20, cex=1.0, col="red")
install.packages("plotly")
plot_ly(data = WA_trees, x = Sepal.Length, y = Petal.Length, mode = "markers")
library(plotly)
plot_ly(data = WA_trees, x = Sepal.Length, y = Petal.Length, mode = "markers")
WA_trees[,"INVYR"]
plot_ly(data = WA_trees, x = WA_trees[,"INVYR"], y = WA_trees[,"TotalCarbon"], mode = "markers")
colnames(WA_trees)
typeof(WA_trees)
plot_ly(data = woof, x = woof[,"INVYR"], y = woof[,"TotalCarbon"], mode = "markers")
#Anna M Calderon
#Matthew K Lau
#Harvard Forest
#gENM-Data Set Up
#Part 0
#8 July 2016
## Step 0. Set a working directory and File Paths
wd <- '../src'
setwd(wd)
## Step 1. Source Helpers Script
source("helpers.R")
## Step 2. Importing Climate Variables for NE
neClim <- stack("../data/neClim.grd")
## Step 3.  Getting climate change projections
# library(maptools)
# vepPolygon <- polygon_from_extent(raster::extent(xmin, xmax, ymin, ymax),
#                                   proj4string="+proj=longlat +ellps=WGS84 +datum=WGS84")
# IDs <- sapply(slot(vepPolygon, "polygons"), function(x) slot(x, "ID"))
# df <- data.frame(rep(0, length(IDs)), row.names=IDs)
# SPDFxx <- SpatialPolygonsDataFrame(vepPolygon, df)
# #tf <- tempfile()
# #writePolyShape(SPDFxx, tf)
# #getinfo.shape(tf)
#
# library(rgdal)
# ## shape <- readOGR('../data/neExtent',layer='neExtent')
# writeOGR(SPDFxx,dsn='../data/neExtent',layer='neExtent',driver='ESRI Shapefile',overwrite_layer=TRUE)
#
## Step 4. Downloading Species Presence Data
gspecies <- ''
prespoints <- read.csv('http://harvardforest.fas.harvard.edu/data/p14/hf147/hf147-13-antData_use4R_snappedToClim.csv')
if (gspecies == ''){gspecies <- "aphrud"}
colnames(prespoints) = c("spcode", "lon","lat")
gspecies <- prespoints[grep(gspecies,as.character(prespoints$spcode)),]
gspecies$spcode <- NULL
if (identical(colnames(gspecies),c( "lat", "lon"))){gspecies <- gspecies[,c('lon','lat')]}
if (is.matrix(gspecies) == FALSE){gspecies <- data.matrix(gspecies)}
## Step 5. Making Clusters and running gENMs
clust <- gClust(x=gspecies, p=neClim$bio1)
gENM(x=gspecies)
#Anna M Calderon
#Matthew K Lau
#Harvard Forest
#gENM-Data Set Up
#Part 0
#8 July 2016
## Step 0. Set a working directory and File Paths
wd <- '../src'
setwd(wd)
## Step 1. Source Helpers Script
source("helpers.R")
## Step 2. Importing Climate Variables for NE
neClim <- stack("../data/neClim.grd")
## Step 3.  Getting climate change projections
# library(maptools)
# vepPolygon <- polygon_from_extent(raster::extent(xmin, xmax, ymin, ymax),
#                                   proj4string="+proj=longlat +ellps=WGS84 +datum=WGS84")
# IDs <- sapply(slot(vepPolygon, "polygons"), function(x) slot(x, "ID"))
# df <- data.frame(rep(0, length(IDs)), row.names=IDs)
# SPDFxx <- SpatialPolygonsDataFrame(vepPolygon, df)
# #tf <- tempfile()
# #writePolyShape(SPDFxx, tf)
# #getinfo.shape(tf)
#
# library(rgdal)
# ## shape <- readOGR('../data/neExtent',layer='neExtent')
# writeOGR(SPDFxx,dsn='../data/neExtent',layer='neExtent',driver='ESRI Shapefile',overwrite_layer=TRUE)
#
## Step 4. Downloading Species Presence Data
gspecies <- ''
prespoints <- read.csv('http://harvardforest.fas.harvard.edu/data/p14/hf147/hf147-13-antData_use4R_snappedToClim.csv')
if (gspecies == ''){gspecies <- "aphrud"}
colnames(prespoints) = c("spcode", "lon","lat")
gspecies <- prespoints[grep(gspecies,as.character(prespoints$spcode)),]
gspecies$spcode <- NULL
if (identical(colnames(gspecies),c( "lat", "lon"))){gspecies <- gspecies[,c('lon','lat')]}
if (is.matrix(gspecies) == FALSE){gspecies <- data.matrix(gspecies)}
## Step 5. Making Clusters and running gENMs
clust <- gClust(x=gspecies, p=neClim$bio1)
gENM(x=gspecies)
source("helpers.R")
source('~/Desktop/gENM/src/helpers.R')
#Anna M Calderon
#Matthew K Lau
#Harvard Forest
#gENM-Data Set Up
#Part 0
#8 July 2016
## Step 0. Set a working directory and File Paths
wd <- '../src'
setwd(wd)
## Step 1. Source Helpers Script
source("helpers.R")
## Step 2. Importing Climate Variables for NE
neClim <- stack("../data/neClim.grd")
## Step 3.  Getting climate change projections
# library(maptools)
# vepPolygon <- polygon_from_extent(raster::extent(xmin, xmax, ymin, ymax),
#                                   proj4string="+proj=longlat +ellps=WGS84 +datum=WGS84")
# IDs <- sapply(slot(vepPolygon, "polygons"), function(x) slot(x, "ID"))
# df <- data.frame(rep(0, length(IDs)), row.names=IDs)
# SPDFxx <- SpatialPolygonsDataFrame(vepPolygon, df)
# #tf <- tempfile()
# #writePolyShape(SPDFxx, tf)
# #getinfo.shape(tf)
#
# library(rgdal)
# ## shape <- readOGR('../data/neExtent',layer='neExtent')
# writeOGR(SPDFxx,dsn='../data/neExtent',layer='neExtent',driver='ESRI Shapefile',overwrite_layer=TRUE)
#
## Step 4. Downloading Species Presence Data
gspecies <- ''
prespoints <- read.csv('http://harvardforest.fas.harvard.edu/data/p14/hf147/hf147-13-antData_use4R_snappedToClim.csv')
if (gspecies == ''){gspecies <- "aphrud"}
colnames(prespoints) = c("spcode", "lon","lat")
gspecies <- prespoints[grep(gspecies,as.character(prespoints$spcode)),]
gspecies$spcode <- NULL
if (identical(colnames(gspecies),c( "lat", "lon"))){gspecies <- gspecies[,c('lon','lat')]}
if (is.matrix(gspecies) == FALSE){gspecies <- data.matrix(gspecies)}
## Step 5. Making Clusters and running gENMs
clust <- gClust(x=gspecies, p=neClim$bio1)
gENM(x=gspecies)
wd <- '../src'
setwd(wd)
#Anna M Calderon
#Matthew K Lau
#Harvard Forest
#gENM-Data Set Up
#Part 0
#8 July 2016
## Step 0. Set a working directory and File Paths
wd <- '/Users/annacalderon/Desktop/gENM/src'
setwd(wd)
## Step 1. Source Helpers Script
source("helpers.R")
## Step 2. Importing Climate Variables for NE
neClim <- stack("../data/neClim.grd")
## Step 3.  Getting climate change projections
# library(maptools)
# vepPolygon <- polygon_from_extent(raster::extent(xmin, xmax, ymin, ymax),
#                                   proj4string="+proj=longlat +ellps=WGS84 +datum=WGS84")
# IDs <- sapply(slot(vepPolygon, "polygons"), function(x) slot(x, "ID"))
# df <- data.frame(rep(0, length(IDs)), row.names=IDs)
# SPDFxx <- SpatialPolygonsDataFrame(vepPolygon, df)
# #tf <- tempfile()
# #writePolyShape(SPDFxx, tf)
# #getinfo.shape(tf)
#
# library(rgdal)
# ## shape <- readOGR('../data/neExtent',layer='neExtent')
# writeOGR(SPDFxx,dsn='../data/neExtent',layer='neExtent',driver='ESRI Shapefile',overwrite_layer=TRUE)
#
## Step 4. Downloading Species Presence Data
gspecies <- ''
prespoints <- read.csv('http://harvardforest.fas.harvard.edu/data/p14/hf147/hf147-13-antData_use4R_snappedToClim.csv')
if (gspecies == ''){gspecies <- "aphrud"}
colnames(prespoints) = c("spcode", "lon","lat")
gspecies <- prespoints[grep(gspecies,as.character(prespoints$spcode)),]
gspecies$spcode <- NULL
if (identical(colnames(gspecies),c( "lat", "lon"))){gspecies <- gspecies[,c('lon','lat')]}
if (is.matrix(gspecies) == FALSE){gspecies <- data.matrix(gspecies)}
clust <- gClust(x=gspecies, p=neClim$bio1)
out <- gENM(x=gspecies, clust=clust)
gAnalysis(x=out)
#Anna M Calderon
#Matthew K Lau
#Harvard Forest
#gENM-Data Set Up
#Part 0
#8 July 2016
## Step 0. Set a working directory and File Paths
wd <- '/Users/annacalderon/Desktop/gENM/src'
setwd(wd)
## Step 1. Source Helpers Script
source("helpers.R")
## Step 2. Importing Climate Variables for NE
neClim <- stack("../data/neClim.grd")
## Step 3.  Getting climate change projections
# library(maptools)
# vepPolygon <- polygon_from_extent(raster::extent(xmin, xmax, ymin, ymax),
#                                   proj4string="+proj=longlat +ellps=WGS84 +datum=WGS84")
# IDs <- sapply(slot(vepPolygon, "polygons"), function(x) slot(x, "ID"))
# df <- data.frame(rep(0, length(IDs)), row.names=IDs)
# SPDFxx <- SpatialPolygonsDataFrame(vepPolygon, df)
# #tf <- tempfile()
# #writePolyShape(SPDFxx, tf)
# #getinfo.shape(tf)
#
# library(rgdal)
# ## shape <- readOGR('../data/neExtent',layer='neExtent')
# writeOGR(SPDFxx,dsn='../data/neExtent',layer='neExtent',driver='ESRI Shapefile',overwrite_layer=TRUE)
#
## Step 4. Downloading Species Presence Data
gspecies <- ''
prespoints <- read.csv('http://harvardforest.fas.harvard.edu/data/p14/hf147/hf147-13-antData_use4R_snappedToClim.csv')
if (gspecies == ''){gspecies <- "aphrud"}
colnames(prespoints) = c("spcode", "lon","lat")
gspecies <- prespoints[grep(gspecies,as.character(prespoints$spcode)),]
gspecies$spcode <- NULL
if (identical(colnames(gspecies),c( "lat", "lon"))){gspecies <- gspecies[,c('lon','lat')]}
if (is.matrix(gspecies) == FALSE){gspecies <- data.matrix(gspecies)}
clust <- gClust(x=gspecies, p=neClim$bio1)
out <- gENM(x=gspecies, clust=clust)
gAnalysis(x=out)
