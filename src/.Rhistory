plot(meow, pch=20, cex=1.0, col="pink")
plot(meow, pch=20, cex=1.0, col="pink", lwd=0.5)
plot(meow, pch=20, cex=1.0, col="pink", lwd=0.5)
plot(meow, pch=20, cex=1.0, col="pink", lwd=0.5)
plot(meow, type="o", pch=20, cex=1.0, col="pink", )
plot(meow, type=1, pch=20, cex=1.0, col="pink", )
points(woof, pch=20, cex=1.0, col="olivegreen")
points(woof, pch=20, cex=1.0, col="olive")
points(woof, pch=20, cex=1.0, col="green")
plot(meow, pch=20, cex=1.0, col="pink" )
points(woof, pch=20, cex=1.0, col="green")
points(woof, pch=20, cex=1.0, col="green")
plot(woof, pch=20, cex=1.0, col="green" )
points(meow, pch=20, cex=1.0, col="red")
install.packages("plotly")
plot_ly(data = WA_trees, x = Sepal.Length, y = Petal.Length, mode = "markers")
library(plotly)
plot_ly(data = WA_trees, x = Sepal.Length, y = Petal.Length, mode = "markers")
WA_trees[,"INVYR"]
plot_ly(data = WA_trees, x = WA_trees[,"INVYR"], y = WA_trees[,"TotalCarbon"], mode = "markers")
colnames(WA_trees)
typeof(WA_trees)
plot_ly(data = woof, x = woof[,"INVYR"], y = woof[,"TotalCarbon"], mode = "markers")
#Anna M Calderon
#Matthew K Lau
#Harvard Forest
#gENM-Data Set Up
#Part 0
#8 July 2016
## Step 0. Set a working directory and File Paths
wd <- '../src'
setwd(wd)
## Step 1. Source Helpers Script
source("helpers.R")
## Step 2. Importing Climate Variables for NE
neClim <- stack("../data/neClim.grd")
## Step 3.  Getting climate change projections
# library(maptools)
# vepPolygon <- polygon_from_extent(raster::extent(xmin, xmax, ymin, ymax),
#                                   proj4string="+proj=longlat +ellps=WGS84 +datum=WGS84")
# IDs <- sapply(slot(vepPolygon, "polygons"), function(x) slot(x, "ID"))
# df <- data.frame(rep(0, length(IDs)), row.names=IDs)
# SPDFxx <- SpatialPolygonsDataFrame(vepPolygon, df)
# #tf <- tempfile()
# #writePolyShape(SPDFxx, tf)
# #getinfo.shape(tf)
#
# library(rgdal)
# ## shape <- readOGR('../data/neExtent',layer='neExtent')
# writeOGR(SPDFxx,dsn='../data/neExtent',layer='neExtent',driver='ESRI Shapefile',overwrite_layer=TRUE)
#
## Step 4. Downloading Species Presence Data
gspecies <- ''
prespoints <- read.csv('http://harvardforest.fas.harvard.edu/data/p14/hf147/hf147-13-antData_use4R_snappedToClim.csv')
if (gspecies == ''){gspecies <- "aphrud"}
colnames(prespoints) = c("spcode", "lon","lat")
gspecies <- prespoints[grep(gspecies,as.character(prespoints$spcode)),]
gspecies$spcode <- NULL
if (identical(colnames(gspecies),c( "lat", "lon"))){gspecies <- gspecies[,c('lon','lat')]}
if (is.matrix(gspecies) == FALSE){gspecies <- data.matrix(gspecies)}
## Step 5. Making Clusters and running gENMs
clust <- gClust(x=gspecies, p=neClim$bio1)
gENM(x=gspecies)
#Anna M Calderon
#Matthew K Lau
#Harvard Forest
#gENM-Data Set Up
#Part 0
#8 July 2016
## Step 0. Set a working directory and File Paths
wd <- '../src'
setwd(wd)
## Step 1. Source Helpers Script
source("helpers.R")
## Step 2. Importing Climate Variables for NE
neClim <- stack("../data/neClim.grd")
## Step 3.  Getting climate change projections
# library(maptools)
# vepPolygon <- polygon_from_extent(raster::extent(xmin, xmax, ymin, ymax),
#                                   proj4string="+proj=longlat +ellps=WGS84 +datum=WGS84")
# IDs <- sapply(slot(vepPolygon, "polygons"), function(x) slot(x, "ID"))
# df <- data.frame(rep(0, length(IDs)), row.names=IDs)
# SPDFxx <- SpatialPolygonsDataFrame(vepPolygon, df)
# #tf <- tempfile()
# #writePolyShape(SPDFxx, tf)
# #getinfo.shape(tf)
#
# library(rgdal)
# ## shape <- readOGR('../data/neExtent',layer='neExtent')
# writeOGR(SPDFxx,dsn='../data/neExtent',layer='neExtent',driver='ESRI Shapefile',overwrite_layer=TRUE)
#
## Step 4. Downloading Species Presence Data
gspecies <- ''
prespoints <- read.csv('http://harvardforest.fas.harvard.edu/data/p14/hf147/hf147-13-antData_use4R_snappedToClim.csv')
if (gspecies == ''){gspecies <- "aphrud"}
colnames(prespoints) = c("spcode", "lon","lat")
gspecies <- prespoints[grep(gspecies,as.character(prespoints$spcode)),]
gspecies$spcode <- NULL
if (identical(colnames(gspecies),c( "lat", "lon"))){gspecies <- gspecies[,c('lon','lat')]}
if (is.matrix(gspecies) == FALSE){gspecies <- data.matrix(gspecies)}
## Step 5. Making Clusters and running gENMs
clust <- gClust(x=gspecies, p=neClim$bio1)
gENM(x=gspecies)
source("helpers.R")
source('~/Desktop/gENM/src/helpers.R')
#Anna M Calderon
#Matthew K Lau
#Harvard Forest
#gENM-Data Set Up
#Part 0
#8 July 2016
## Step 0. Set a working directory and File Paths
wd <- '../src'
setwd(wd)
## Step 1. Source Helpers Script
source("helpers.R")
## Step 2. Importing Climate Variables for NE
neClim <- stack("../data/neClim.grd")
## Step 3.  Getting climate change projections
# library(maptools)
# vepPolygon <- polygon_from_extent(raster::extent(xmin, xmax, ymin, ymax),
#                                   proj4string="+proj=longlat +ellps=WGS84 +datum=WGS84")
# IDs <- sapply(slot(vepPolygon, "polygons"), function(x) slot(x, "ID"))
# df <- data.frame(rep(0, length(IDs)), row.names=IDs)
# SPDFxx <- SpatialPolygonsDataFrame(vepPolygon, df)
# #tf <- tempfile()
# #writePolyShape(SPDFxx, tf)
# #getinfo.shape(tf)
#
# library(rgdal)
# ## shape <- readOGR('../data/neExtent',layer='neExtent')
# writeOGR(SPDFxx,dsn='../data/neExtent',layer='neExtent',driver='ESRI Shapefile',overwrite_layer=TRUE)
#
## Step 4. Downloading Species Presence Data
gspecies <- ''
prespoints <- read.csv('http://harvardforest.fas.harvard.edu/data/p14/hf147/hf147-13-antData_use4R_snappedToClim.csv')
if (gspecies == ''){gspecies <- "aphrud"}
colnames(prespoints) = c("spcode", "lon","lat")
gspecies <- prespoints[grep(gspecies,as.character(prespoints$spcode)),]
gspecies$spcode <- NULL
if (identical(colnames(gspecies),c( "lat", "lon"))){gspecies <- gspecies[,c('lon','lat')]}
if (is.matrix(gspecies) == FALSE){gspecies <- data.matrix(gspecies)}
## Step 5. Making Clusters and running gENMs
clust <- gClust(x=gspecies, p=neClim$bio1)
gENM(x=gspecies)
wd <- '../src'
setwd(wd)
library(geoknife)
stencil <- webgeom()
stencil
query(stencil, 'geoms')
?webgeom
query('webdata')
library(geoknife)
stencil <- webgeom()
query(stencil,'geoms')
geom(stencil) <- 'sample:CONUS_Climate_Divisions'
query(stencil, 'attributes')
attribute(stencil) <- 'NAME'
query(stencil, 'values')
values(stencil) <- 'EAST CENTRAL MOUNTAINS'
query('webdata')
fabric <- webdata(url='dods://cida.usgs.gov/thredds/dodsC/macav2metdata_daily_future')
query(fabric,'variables')
variables(fabric) <- 'tasmax_CSIRO-Mk3-6-0_r1i1p1_rcp85'
query(fabric,'times')
times(fabric) <- c('2006-01-01', '2007-01-01')
job <- geoknife(stencil, fabric, wait=TRUE)
data.out <- result(job, with.units=TRUE)
data.out
colnames(data.out)
typeof(data.out)
plot(data.out$variable)
plot(data.out$`EAST CENTRAL MOUNTAINS`)
rasterImage(data.out$`EAST CENTRAL MOUNTAINS`)
data.out[1,]
?result
library(geoknife)
stencil <- webgeom()
query(stencil,'geoms')
geom(stencil) <- 'sample:CONUS_Climate_Divisions'
query(stencil, 'attributes')
attribute(stencil) <- 'NAME'
query(stencil, 'values')
values(stencil) <- 'EAST CENTRAL MOUNTAINS'
query('webdata')
fabric <- webdata(url='dods://cida.usgs.gov/thredds/dodsC/macav2metdata_daily_future')
query(fabric,'variables')
variables(fabric) <- 'tasmax_CSIRO-Mk3-6-0_r1i1p1_rcp85'
query(fabric,'times')
times(fabric) <- c('2006-01-01', '2006-03-01')
job <- geoknife(stencil, fabric, wait=TRUE)
data.out <- result(job, with.units=TRUE)
data.out
library(geoknife)
stencil <- webgeom()
query(stencil,'geoms')
geom(stencil) <- 'sample:CONUS_Climate_Divisions'
query(stencil, 'attributes')
attribute(stencil) <- 'NAME'
query(stencil, 'values')
values(stencil) <- 'EAST CENTRAL MOUNTAINS'
query('webdata')
fabric <- webdata(url='dods://cida.usgs.gov/thredds/dodsC/macav2metdata_daily_future')
query(fabric,'variables')
variables(fabric) <- 'tasmax_CSIRO-Mk3-6-0_r1i1p1_rcp85'
query(fabric,'times')
times(fabric) <- c('2006-01-01', '2006-03-01')
?job
?geoknife
job <- geoknife(stencil, fabric, wait = TRUE, OUTPUT_TYPE = "geotiff")
?webprocess
?fabric
library(geoknife)
library(geoknife)
knife <- webprocess(algorithm = list('OPeNDAP Subset'="gov.usgs.cida.gdp.wps.
algorithm.FeatureCoverageOPeNDAPIntersectionAlgorithm"))
webprocess()
?webprocess
knife <- webprocess()
query(knife, 'algorithms')
knife <- webprocess(algorithm = list('Area Grid Statistics (weighted)'=
"gov.usgs.cida.gdp.wps.algorithm.FeatureWeightedGridStatisticsAlgorithm"))
fabric <- webdata(url='dods://cida.usgs.gov/thredds/dodsC/macav2metdata_daily_future')
stencil <- simplegeom(data.frame('point1' = c(-73,41), 'point2' = c(-66.95833,-47.45833)))
fabric <- webdata(url='dods://cida.usgs.gov/thredds/dodsC/macav2metdata_daily_future',
variable='tasmax_CSIRO-Mk3-6-0_r1i1p1_rcp85',times=c('2014-07-15','2014-07-18'))
stencil <- simplegeom(data.frame('point1' = c(-73,41), 'point2' = c(-66.95833,-47.45833)))
job <- geoknife(stencil, fabric, knife, wait = TRUE, OUTPUT_TYPE = "geotiff")
?OUTPUT_TYPE
job <- geoknife(stencil, fabric, knife, wait = TRUE)
download(job, destination = file.path(tempdir(), 'geoknife_data.zip'), overwrite=TRUE)
download(job, destination = "/Users/annacalderon/Desktop/geoknife_data.zip", overwrite=TRUE)
unzip(file, exdir=file.path(tempdir(),'NASA'))
knife <- webprocess(algorithm = list('Area Grid Statistics (weighted)'=
"gov.usgs.cida.gdp.wps.algorithm.FeatureWeightedGridStatisticsAlgorithm")
email = 'acalderon@ucmerced.edu')
knife <- webprocess(algorithm = list('Area Grid Statistics (weighted)'=
"gov.usgs.cida.gdp.wps.algorithm.FeatureWeightedGridStatisticsAlgorithm")
email = acalderon@ucmerced.edu)
knife <- webprocess(email = 'acalderon@ucmerced.edu')
knife
knife <- webprocess(wait = TRUE)
knife
knife <- webprocess(algorithm = list('Area Grid Statistics (weighted)'=
"gov.usgs.cida.gdp.wps.algorithm.FeatureWeightedGridStatisticsAlgorithm"))
knife <- webprocess(email = 'acalderon@ucmerced.edu')
knife <- webprocess(wait = TRUE)
knife
knife <- webprocess(email = 'acalderon@ucmerced.edu')
knife
knife <- webprocess(algorithm = list('Area Grid Statistics (weighted)'=
"gov.usgs.cida.gdp.wps.algorithm.FeatureWeightedGridStatisticsAlgorithm"))
knife <- webprocess(wait = TRUE)
job <- geoknife(stencil, fabric, knife, wait = TRUE)
check(job)
stencil <- webgeom('state::Rhode Island, Massachusetts, Maine, Vermont, Connecticut, New Hampshire')
job <- geoknife(stencil, fabric, knife, wait = TRUE)
check(job)
job <- geoknife(stencil, fabric, knife, wait = TRUE, OUTPUT_TYPE="geotiff")
check(job)
download(job, destination = "/Users/annacalderon/Desktop/", overwrite=TRUE)
knife <- webprocess(algorithm = list('Area Grid Statistics (weighted)'=
"gov.usgs.cida.gdp.wps.algorithm.FeatureWeightedGridStatisticsAlgorithm"))
knife <- webprocess(email = 'acalderon@ucmerced.edu')
fabric <- webdata(url='dods://cida.usgs.gov/thredds/dodsC/macav2metdata_daily_future',
variable='tasmax_CSIRO-Mk3-6-0_r1i1p1_rcp85',times=c('2014-07-15','2014-07-18'))
stencil <- webgeom('state::Rhode Island, Massachusetts, Maine, Vermont, Connecticut, New Hampshire')
job <- geoknife(stencil, fabric, knife, wait = TRUE)
check(job)
knife <- webprocess(algorithm = list('Area Grid Statistics (weighted)'=
"gov.usgs.cida.gdp.wps.algorithm.FeatureWeightedGridStatisticsAlgorithm"))
knife <- webprocess(wait = TRUE)
fabric <- webdata(url='dods://cida.usgs.gov/thredds/dodsC/macav2metdata_daily_future',
variable='tasmax_CSIRO-Mk3-6-0_r1i1p1_rcp85',times=c('2014-07-15','2014-07-18'))
stencil <- webgeom('state::Rhode Island, Massachusetts, Maine, Vermont, Connecticut, New Hampshire')
job <- geoknife(stencil, fabric, knife, wait = TRUE)
download(job, destination = "/Users/annacalderon/Desktop/", overwrite=TRUE)
download(job, destination = "/Users/annacalderon/Desktop", overwrite=TRUE)
check(job)
geom(stencil) <- "derivative:CONUS_States"
attribute(stencil) <- "STATE"
values(stencil) <- c("Rhode Island", "Massachusetts", "Maine", "Vermont", "Connecticut", "New Hampshire")
stencil
geom(stencil) <- "sample:CONUS_Climate_Divisions""
geom(stencil) <- "sample:CONUS_Climate_Divisions")
geom(stencil) <- "sample:CONUS_Climate_Divisions"
attribute(stencil) <- "STATE"
values(stencil) <- c("Rhode Island", "Massachusetts", "Maine", "Vermont", "Connecticut", "New Hampshire")
job <- geoknife(stencil, fabric, knife, wait = TRUE)
check(job)
geom(stencil) <- "sample:CONUS_Climate_Divisions"
attribute(stencil) <- "STATE"
values(stencil) <- c("Rhode Island", "Massachusetts")
job <- geoknife(stencil, fabric, knife, wait = TRUE)
check(job)
geom(stencil) <- "sample:CONUS_Climate_Divisions"
attribute(stencil) <- "STATE"
values(stencil) <- c("Rhode Island", "Massachusetts", "Maine")
job <- geoknife(stencil, fabric, knife, wait = TRUE)
check(job)
geom(stencil) <- "sample:CONUS_Climate_Divisions"
attribute(stencil) <- "STATE"
values(stencil) <- c("Rhode Island", "Massachusetts", "Vermont", "Connecticut")
job <- geoknife(stencil, fabric, knife, wait = TRUE)
check(job)
values(stencil) <- c("Maine")
job <- geoknife(stencil, fabric, knife, wait = TRUE)
check(job)
geom(stencil) <- "sample:CONUS_Climate_Divisions")
attribute(stencil) <- "STATE"
values(stencil) <- c("Rhode Island", "Massachusetts", "Vermont", "Connecticut", "New Hampshire")
job <- geoknife(stencil, fabric, knife, wait = TRUE)
check(job)
data.out <- result(job, with.units=TRUE)
data.out
typeof(data.out)
knife <- webprocess(algorithm = list('OPeNDAP Subset'="gov.usgs.cida.gdp.wps.algorithm.FeatureCoverageOPeNDAPIntersectionAlgorithm"))
fabric <- webdata(url='dods://opendap.larc.nasa.gov/opendap/hyrax/SortByProduct/CERES/EBAF/Surface_Edition2.8/CERES_EBAF-Surface_Edition2.8_200003-201506.nc',
variable="sfc_sw_down_all_mon", #Surface Shortwave Flux Down, Monthly Means, All-Sky conditions
times=c('2014-07-15','2014-07-15'))
stencil <- simplegeom(data.frame('point1' = c(-5,32), 'point2' = c(-90,-78))) # big 'ol chunk 'o data
job <- geoknife(stencil, fabric, knife, wait = TRUE, OUTPUT_TYPE = "geotiff")
file <- download(job, destination = file.path(tempdir(), 'nasa_data.zip'), overwrite=TRUE)
job
file <- download(job, destination = file.path(tempdir(), 'nasa_data.zip'), overwrite=TRUE)
data.out <- result(job, with.units=TRUE)
job <- geoknife(stencil, fabric, knife, wait = TRUE)
library(geoknife)
knife <- webprocess(algorithm = list('Area Grid Statistics (weighted)'=
"gov.usgs.cida.gdp.wps.algorithm.FeatureWeightedGridStatisticsAlgorithm"))
knife <- webprocess(wait = TRUE)
fabric <- webdata(url='dods://cida.usgs.gov/thredds/dodsC/macav2metdata_daily_future',
variable='tasmax_CSIRO-Mk3-6-0_r1i1p1_rcp85',times=c('2014-07-15','2014-07-18'))
geom(stencil) <- "sample:CONUS_Climate_Divisions")
geom(stencil) <- "sample:CONUS_Climate_Divisions"
geom(stencil) <- "sample:CONUS_Climate_Divisions"
library(geoknife)
knife <- webprocess(algorithm = list('Area Grid Statistics (weighted)'=
"gov.usgs.cida.gdp.wps.algorithm.FeatureWeightedGridStatisticsAlgorithm"))
knife <- webprocess(wait = TRUE)
fabric <- webdata(url='dods://cida.usgs.gov/thredds/dodsC/macav2metdata_daily_future',
variable='tasmax_CSIRO-Mk3-6-0_r1i1p1_rcp85',times=c('2014-07-15','2014-07-18'))
geom(stencil) <- "sample:CONUS_Climate_Divisions"
attribute(stencil) <- "STATE"
values(stencil) <- c("Rhode Island", "Massachusetts", "Vermont", "Connecticut", "New Hampshire")
job <- geoknife(stencil, fabric, knife, wait = TRUE)
rm(list=ls())
library(geoknife)
knife <- webprocess(algorithm = list('Area Grid Statistics (weighted)'=
"gov.usgs.cida.gdp.wps.algorithm.FeatureWeightedGridStatisticsAlgorithm"))
knife <- webprocess(wait = TRUE)
fabric <- webdata(url='dods://cida.usgs.gov/thredds/dodsC/macav2metdata_daily_future',
variable='tasmax_CSIRO-Mk3-6-0_r1i1p1_rcp85',times=c('2014-07-15','2014-07-18'))
geom(stencil) <- "sample:CONUS_Climate_Divisions"
> library(raster)
str_name <- '/Users/annacalderon/Desktop/gENM/data/
library(raster)
library(raster)
str_name <- '/Users/annacalderon/Desktop/gENM/data/
cida.usgs.gov-thredds-dodsC-macav2metdata_daily_future-2099-07-01-00-00-00.tiff")
)
str_name <- '/Users/annacalderon/Desktop/gENM/data/
cida.usgs.gov-thredds-dodsC-macav2metdata_daily_future-2099-07-01-00-00-00.tiff')
str_name <- '/Users/annacalderon/Desktop/gENM/data/try.tiff"
str_name <- ('/Users/annacalderon/Desktop/gENM/data/try.tiff')
raster(str_name)
herewego <-raster(str_name)
plot(herewego)
wd <- '/Users/annacalderon/Desktop/gENM/src'
setwd(wd)
## Step 1. Source Helpers Script
source("helpers.R")
mintemp.2006  <- raster("../data/01_01_2006.tiff")
gsp <-read.csv("../data/RICTMEdukesnantucket.csv")
if (is.matrix(gsp) == FALSE){gsp <- data.matrix(gsp)}
x=gsp
p=mintemp.2006
c.rad=50000
seed=123
n=1000
set.seed(seed)
circ <- circles(x, d=c.rad, lonlat=T)
random <- spsample(circ@polygons, n, type='random', iter=100)
# Makes circles with a 5K radius of each
# point and adds 1000 randomized points.
gsp_bc <-  extract(p, x)
head(gsp)
head(gsp_bc)
as.matrix(gsp_bc)
gsp_bc <- as.matrix(gsp_bc)
gsp_bc <-  data.frame(cbind(x,gsp_bc))
gsp_bc
random_bc <- extract(p, random)
head(random_bc)
random  <- random@coords
colnames(random) <- c("lon","lat")
random_bc <- as.matrix(random_bc)
head(random_bc)
random_bc <-  data.frame(cbind(random,random_bc))
head(random_bc)
random_bc  <-  random_bc[!is.na(random_bc[,3]), ]
me <- maxent(p, gsp_bc[,c("lon", "lat")], random_bc[,c("lon", "lat")])
cat  <- c(1,2,3,4,5,6,7,8,9)
dog  <- c(1,2,3,4,5,6,7,8,9)
cbind(cat, dog)
data.frame(cbind(cat, dog))
gsp_bc <-  extract(p, x)
install.packages("gtools")
library(gtools)
gsp_bc <-  extract(p, x)
?smartbind
gsp_bc <-  data.frame(smartbind(x,gsp_bc))
gsp_bc
head(gsp_bc)
smartbind(x,gsp_bc)
head(smartbind(x,gsp_bc))
remove.packages("gtools", lib="~/Library/R/3.2/library")
rbind( gsp , setNames( rev(gsp) , names( gsp_bc ) ) )
cbind( gsp , setNames( rev(gsp) , names( gsp_bc ) ) )
diditwork  <- data.frame(mapply(c(gsp_bc,gsp[,c(1,2)]),SIMPLIFY=F))
diditwork  <- data.frame(mapply(c(gsp_bc,gsp[,c("lon","lat")]),SIMPLIFY=F))
gsp[,c("lon","lat")])
gsp[,c("lon","lat")]
diditwork  <- data.frame(mapply(c(gsp_bc, gsp[,c("lon","lat")]),SIMPLIFY=F))
gsp[,"lat"]
gsplat  <- as.data.frame(gsp[,"lat"])
gsplat
head(gsplat)
colnames(gsplat)
colnames(gsplat)  <- NULL
colnames(gsplat)
head(gsplat)
gsplon  <- as.data.frame(gsp[,"lon"])
colnames(gsplon) <- NULL
gsp_bc
gsp_bc <-  extract(p, x)
gsp_bc
head(gsp_bc)
gsp_bc <-  data.frame(cbind(gslplon,gsplat ,gsp_bc))
gsp_bc <-  data.frame(cbind(gspplon,gsplat ,gsp_bc))
gsp_bc <-  data.frame(cbind(gsplon,gsplat ,gsp_bc))
gsp_bc
head(gsp_bc)
colnames(gsp_bc)  <- NULL
head(gsp_bc)
colnames(gsplat) <- NA
head(gsplat)
colanames(gsplon)  <- NA
colnames(gsplon)  <- NA
colnames(gsp_bc)  <- NA
gsp_bc <-  data.frame(cbind(gsplon,gsplat, gsp_bc))
head(gsp_bc)
gsp <-read.csv("../data/RICTMEdukesnantucket.csv")
if (is.matrix(gsp) == FALSE){gsp <- data.matrix(gsp)}
x=gsp
mintemp.2006  <- raster("../data/01_01_2006.tiff")
p=mintemp.2006
c.rad=50000
seed=123
n=1000
set.seed(seed)
circ <- circles(x, d=c.rad, lonlat=T)
random <- spsample(circ@polygons, n, type='random', iter=100)
gsp_bc <-  extract(p, x)
colnames(gsp_bc) <- NA
head(gsp_bc)
data.frame(gsp_bc)
gsp_bc <- data.frame(gsp_bc)
colnames(gsp_bc) <- NA
gsplat <- data.frame(gsp[,"lat"])
colnames(gsplat) <- NA
data.frame(gsp_bc)
head(gsplat)
head(gsp_bc)
gsplon <- data.frame(gsp[,"lon"])
colnames(gsplon) <- NA
gsp_bc <-  data.frame(cbind(gsplon,gsplat,gsp_bc))
head(gsp_bc)
colnames(gsp_bc)
colnames(gsplon) <- "NA"
gsp_bc <-  extract(p, x)
gsp_bc <- data.frame(gsp_bc)
colnames(gsp_bc) <- "NA"
gsplat <- data.frame(gsp[,"lat"])
colnames(gsplat) <- "NA"
gsplon <- data.frame(gsp[,"lon"])
colnames(gsplon) <- "NA"
gsp_bc <-  data.frame(cbind(gsplon,gsplat,gsp_bc))
head(gsp_bc)
colnames(gsp_bc)
identical(names(gsp_bc$NA.), names(gsp_bc$NA..1))
is.identical(names(gsp_bc$NA.), names(gsp_bc$NA..1))
random_bc <- data.frame(random_bc)
random_bc <- extract(p, random)
colnames(random_bc) <- "NA"
random_bc <- data.frame(random_bc)
head(random_bc)
colnames(random_bc) <- "NA"
random  <- random@coords
colnames(random) <- c("lon","lat")
randlat <- data.frame(random[,"lat"])
colnames(randlat) <- "NA"
randlon <- data.frame(random[,"lon"])
colnames(randlon) <- "NA"
random_bc <-  data.frame(cbind(randomlon, randlat,random_bc))
random_bc <-  data.frame(cbind(randlon, randlat,random_bc))
random_bc  <-  random_bc[!is.na(random_bc[,3]), ]
head(random_bc)
head(p)
me <- maxent(p, gsp_bc[,c("lon", "lat")], random_bc[,c("lon", "lat")])
me <- maxent(gsp_bc$NA..2, gsp_bc[,(1:2)], random_bc[,(1:2)])
